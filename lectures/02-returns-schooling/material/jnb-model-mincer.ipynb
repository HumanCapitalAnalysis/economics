{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mincer returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pd.options.display.float_format = '${:,.2f}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by simulating a dataset based on the accounting identity model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_0 = 239.15215950404396\n",
    "kappa = 1.0\n",
    "rho_0 = 0.075\n",
    "rho_s = 0.1250\n",
    "T = 55\n",
    "num_agents = 1000\n",
    "\n",
    "\n",
    "\n",
    "def log_observed_earnings(s, x):\n",
    "    \"\"\"This function simulates logarithmic earnings directly from the accounting-identify model.\"\"\"\n",
    "    rslt = 0\n",
    "    rslt += np.log(P_0) - kappa\n",
    "    rslt += rho_s * s\n",
    "    rslt += (rho_0 * kappa + (rho_0*kappa)/ (2 * T) + kappa / T) * x\n",
    "    rslt -= (rho_0 * kappa / (2 * T)) * (x ** 2) + np.random.normal(scale=0.1)\n",
    "\n",
    "    return rslt\n",
    "\n",
    "data = []\n",
    "for i in range(num_agents):\n",
    "    s = np.random.choice(range(10, 16))\n",
    "    x = np.random.choice(range(1, T))\n",
    "    y = log_observed_earnings(s, x)\n",
    "    age = s + x + 6\n",
    "    \n",
    "    data += [[i, age, np.exp(y), s, x]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to store the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Identifier', 'Age', 'Earnings', 'Schooling', 'Experience']\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "df.set_index('Identifier', inplace=True)\n",
    "df.to_pickle('data.mincer.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now load our simulated dataset and run the conventional Mincer regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Earnings</th>\n",
       "      <th>Schooling</th>\n",
       "      <th>Experience</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Identifier</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>55</td>\n",
       "      <td>$3,866.34</td>\n",
       "      <td>13</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27</td>\n",
       "      <td>$776.89</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>65</td>\n",
       "      <td>$7,997.17</td>\n",
       "      <td>15</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70</td>\n",
       "      <td>$7,506.51</td>\n",
       "      <td>13</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>58</td>\n",
       "      <td>$4,405.83</td>\n",
       "      <td>10</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Age  Earnings  Schooling  Experience\n",
       "Identifier                                      \n",
       "0            55 $3,866.34         13          36\n",
       "1            27   $776.89         11          10\n",
       "2            65 $7,997.17         15          44\n",
       "3            70 $7,506.51         13          51\n",
       "4            58 $4,405.83         10          42"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle('data.mincer.pkl')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run the baseline regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>    <td>np.log(Earnings)</td> <th>  R-squared:         </th> <td>   0.987</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.987</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>2.499e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 29 Jun 2018</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>11:48:59</td>     <th>  Log-Likelihood:    </th> <td>  874.72</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  1000</td>      <th>  AIC:               </th> <td>  -1741.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   996</td>      <th>  BIC:               </th> <td>  -1722.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     3</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "            <td></td>               <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>             <td>    4.4797</td> <td>    0.025</td> <td>  176.650</td> <td> 0.000</td> <td>    4.430</td> <td>    4.529</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Schooling</th>             <td>    0.1251</td> <td>    0.002</td> <td>   69.042</td> <td> 0.000</td> <td>    0.122</td> <td>    0.129</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Experience</th>            <td>    0.0920</td> <td>    0.001</td> <td>  108.370</td> <td> 0.000</td> <td>    0.090</td> <td>    0.094</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.square(Experience)</th> <td>   -0.0007</td> <td>  1.5e-05</td> <td>  -44.821</td> <td> 0.000</td> <td>   -0.001</td> <td>   -0.001</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 0.210</td> <th>  Durbin-Watson:     </th> <td>   1.866</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.900</td> <th>  Jarque-Bera (JB):  </th> <td>   0.294</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.009</td> <th>  Prob(JB):          </th> <td>   0.863</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.918</td> <th>  Cond. No.          </th> <td>1.05e+04</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:       np.log(Earnings)   R-squared:                       0.987\n",
       "Model:                            OLS   Adj. R-squared:                  0.987\n",
       "Method:                 Least Squares   F-statistic:                 2.499e+04\n",
       "Date:                Fri, 29 Jun 2018   Prob (F-statistic):               0.00\n",
       "Time:                        11:48:59   Log-Likelihood:                 874.72\n",
       "No. Observations:                1000   AIC:                            -1741.\n",
       "Df Residuals:                     996   BIC:                            -1722.\n",
       "Df Model:                           3                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "=========================================================================================\n",
       "                            coef    std err          t      P>|t|      [0.025      0.975]\n",
       "-----------------------------------------------------------------------------------------\n",
       "Intercept                 4.4797      0.025    176.650      0.000       4.430       4.529\n",
       "Schooling                 0.1251      0.002     69.042      0.000       0.122       0.129\n",
       "Experience                0.0920      0.001    108.370      0.000       0.090       0.094\n",
       "np.square(Experience)    -0.0007    1.5e-05    -44.821      0.000      -0.001      -0.001\n",
       "==============================================================================\n",
       "Omnibus:                        0.210   Durbin-Watson:                   1.866\n",
       "Prob(Omnibus):                  0.900   Jarque-Bera (JB):                0.294\n",
       "Skew:                          -0.009   Prob(JB):                        0.863\n",
       "Kurtosis:                       2.918   Cond. No.                     1.05e+04\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.05e+04. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formula = 'np.log(Earnings) ~ Schooling + Experience + np.square(Experience)'\n",
    "model = smf.ols(formula=formula, data=df)\n",
    "model.fit().summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results are designed so that they line up rather closely with the estimated coeffiecients reported in Table 2 for Whites in 1940."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
