{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mincer returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pd.options.display.float_format = '${:,.2f}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by simulating a dataset based on the accounting identity model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_0 = 239.15215950404396\n",
    "kappa = 1.0\n",
    "rho_0 = 0.075\n",
    "rho_s = 0.1250\n",
    "T = 55\n",
    "num_agents = 1000\n",
    "\n",
    "\n",
    "\n",
    "def log_observed_earnings(s, x):\n",
    "    \"\"\"This function simulates logarithmic earnings directly from the accounting-identify model.\"\"\"\n",
    "    rslt = 0\n",
    "    rslt += np.log(P_0) - kappa\n",
    "    rslt += rho_s * s\n",
    "    rslt += (rho_0 * kappa + (rho_0*kappa)/ (2 * T) + kappa / T) * x\n",
    "    rslt -= (rho_0 * kappa / (2 * T)) * (x ** 2) + np.random.normal(scale=0.1)\n",
    "\n",
    "    return rslt\n",
    "\n",
    "data = []\n",
    "for i in range(num_agents):\n",
    "    s = np.random.choice(range(10, 16))\n",
    "    x = np.random.choice(range(1, T))\n",
    "    y = log_observed_earnings(s, x)\n",
    "    age = s + x + 6\n",
    "    \n",
    "    data += [[i, age, np.exp(y), s, x]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to store the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Identifier', 'Age', 'Earnings', 'Schooling', 'Experience']\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "df.set_index('Identifier', inplace=True)\n",
    "df.to_pickle('data.mincer.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now load our simulated dataset and run the conventional Mincer regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Earnings</th>\n",
       "      <th>Schooling</th>\n",
       "      <th>Experience</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Identifier</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>$5,841.26</td>\n",
       "      <td>11</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33</td>\n",
       "      <td>$1,643.54</td>\n",
       "      <td>15</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19</td>\n",
       "      <td>$398.34</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30</td>\n",
       "      <td>$1,149.03</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>67</td>\n",
       "      <td>$8,965.13</td>\n",
       "      <td>14</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Age  Earnings  Schooling  Experience\n",
       "Identifier                                      \n",
       "0            63 $5,841.26         11          46\n",
       "1            33 $1,643.54         15          12\n",
       "2            19   $398.34         11           2\n",
       "3            30 $1,149.03         10          14\n",
       "4            67 $8,965.13         14          47"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle('data.mincer.pkl')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run the baseline regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>    <td>np.log(Earnings)</td> <th>  R-squared:         </th> <td>   0.988</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.987</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>2.625e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Tue, 29 May 2018</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>08:33:46</td>     <th>  Log-Likelihood:    </th> <td>  866.99</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  1000</td>      <th>  AIC:               </th> <td>  -1726.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   996</td>      <th>  BIC:               </th> <td>  -1706.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     3</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "            <td></td>               <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>             <td>    4.4705</td> <td>    0.025</td> <td>  176.035</td> <td> 0.000</td> <td>    4.421</td> <td>    4.520</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Schooling</th>             <td>    0.1241</td> <td>    0.002</td> <td>   67.110</td> <td> 0.000</td> <td>    0.120</td> <td>    0.128</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Experience</th>            <td>    0.0956</td> <td>    0.001</td> <td>  113.649</td> <td> 0.000</td> <td>    0.094</td> <td>    0.097</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.square(Experience)</th> <td>   -0.0007</td> <td> 1.49e-05</td> <td>  -48.079</td> <td> 0.000</td> <td>   -0.001</td> <td>   -0.001</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 1.443</td> <th>  Durbin-Watson:     </th> <td>   2.036</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.486</td> <th>  Jarque-Bera (JB):  </th> <td>   1.342</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.086</td> <th>  Prob(JB):          </th> <td>   0.511</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 3.054</td> <th>  Cond. No.          </th> <td>1.03e+04</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:       np.log(Earnings)   R-squared:                       0.988\n",
       "Model:                            OLS   Adj. R-squared:                  0.987\n",
       "Method:                 Least Squares   F-statistic:                 2.625e+04\n",
       "Date:                Tue, 29 May 2018   Prob (F-statistic):               0.00\n",
       "Time:                        08:33:46   Log-Likelihood:                 866.99\n",
       "No. Observations:                1000   AIC:                            -1726.\n",
       "Df Residuals:                     996   BIC:                            -1706.\n",
       "Df Model:                           3                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "=========================================================================================\n",
       "                            coef    std err          t      P>|t|      [0.025      0.975]\n",
       "-----------------------------------------------------------------------------------------\n",
       "Intercept                 4.4705      0.025    176.035      0.000       4.421       4.520\n",
       "Schooling                 0.1241      0.002     67.110      0.000       0.120       0.128\n",
       "Experience                0.0956      0.001    113.649      0.000       0.094       0.097\n",
       "np.square(Experience)    -0.0007   1.49e-05    -48.079      0.000      -0.001      -0.001\n",
       "==============================================================================\n",
       "Omnibus:                        1.443   Durbin-Watson:                   2.036\n",
       "Prob(Omnibus):                  0.486   Jarque-Bera (JB):                1.342\n",
       "Skew:                           0.086   Prob(JB):                        0.511\n",
       "Kurtosis:                       3.054   Cond. No.                     1.03e+04\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.03e+04. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formula = 'np.log(Earnings) ~ Schooling + Experience + np.square(Experience)'\n",
    "model = smf.ols(formula=formula, data=df)\n",
    "model.fit().summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results are designed so that they line up rather closely with the estimated coeffiecients reported in Table 2 for Whites in 1940."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## True Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.79530393, 0.74772136, 0.67040445, 0.51993881])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import lognorm\n",
    "from scipy.stats import norm\n",
    "\n",
    "\n",
    "\n",
    "Y_s = np.tile(np.nan, 5)\n",
    "V_s = np.tile(np.nan, 5)\n",
    "p_s = np.tile(np.nan, 4)\n",
    "\n",
    "r = 0.1\n",
    "s = 0.1\n",
    "\n",
    "Y_s[0] = 1\n",
    "for i in range(1, 5):\n",
    "    Y_s[i] = Y_s[i - 1] * (1 + r)\n",
    "\n",
    "\n",
    "# This is the adjustment to the expected earnings\n",
    "shift = np.exp(s ** 2 / 2)\n",
    "\n",
    "# Value of choosing five years of schooling from the perspective of four years.\n",
    "V_s[4] = Y_s[4] * shift\n",
    "\n",
    "# Value of choosing four years of schooling from the perspective of three years.\n",
    "eval_point = V_s[4]/ ((1 + r) * Y_s[3])\n",
    "p_s[3] = lognorm.cdf(eval_point, s)\n",
    "\n",
    "V_s[3] = 0\n",
    "V_s[3] += (1 - p_s[3]) * Y_s[3] * conditional_expectation(eval_point, s) \n",
    "V_s[3] += p_s[3] * (V_s[4] / (1 + r))\n",
    "\n",
    "# Value of choosing three years of schooling from teh perspective of two\n",
    "eval_point = V_s[3] / ((1 + r) * Y_s[2]) \n",
    "p_s[2] = lognorm.cdf(eval_point, s)\n",
    "\n",
    "V_s[2] = 0\n",
    "V_s[2] += (1 - p_s[2]) * Y_s[2] * conditional_expectation(eval_point, s)\n",
    "V_s[2] += p_s[2] * (V_s[3] / (1 + r))\n",
    "\n",
    "# Value of choosing two years of schooling from the perspective of two\n",
    "eval_point = V_s[2] / ((1 + r) * Y_s[1]) \n",
    "p_s[1] = lognorm.cdf(eval_point, s)\n",
    "\n",
    "V_s[1] = 0\n",
    "V_s[1] += (1 - p_s[1]) * Y_s[1] * conditional_expectation(eval_point, s)\n",
    "V_s[1] += p_s[1] * (V_s[2] / (1 + r))\n",
    "\n",
    "# Value of choosing one year of schooling from teh perspective of zero\n",
    "eval_point = V_s[1] / ((1 + r) * Y_s[0]) \n",
    "p_s[0] = lognorm.cdf(eval_point, s)\n",
    "\n",
    "V_s[0] = 0\n",
    "V_s[0] += (1 - p_s[0]) * Y_s[0] * conditional_expectation(eval_point, s) \n",
    "V_s[0] += p_s[0] * (V_s[1] / (1 + r))\n",
    "p_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
